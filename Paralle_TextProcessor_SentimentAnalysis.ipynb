{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh_2HMBgJdD1",
        "outputId": "ef22e720-660a-445e-ec95-c0ce2528cf3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting backend.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile backend.py\n",
        "# ============================================================\n",
        "# üß† backend.py ‚Äî Twitter Sentiment Analysis helpers (fixed)\n",
        "# ============================================================\n",
        "# This file defines functions only (no top-level file upload or heavy work).\n",
        "# Call the functions from your Streamlit UI (app.py).\n",
        "# If executed directly (python backend.py) it will run the full pipeline.\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import sqlite3\n",
        "import multiprocessing\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.image import MIMEImage\n",
        "from email.mime.base import MIMEBase\n",
        "from email import encoders\n",
        "import nltk\n",
        "import spacy\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# -- Ensure minimal NLTK downloads (quiet)\n",
        "nltk.download(\"vader_lexicon\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Configuration defaults\n",
        "# ------------------------------------------------------------\n",
        "DEFAULT_CSV = \"tweetsData.csv\"\n",
        "DEFAULT_DB = \"tweets.db\"\n",
        "SENT_PIE_PATH = \"sentiment_pie.png\"\n",
        "OUTPUT_CSV = \"Milestone3_Final_Results.csv\"\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Utility: load CSV\n",
        "# ------------------------------------------------------------\n",
        "def load_csv(csv_path=DEFAULT_CSV):\n",
        "    \"\"\"Load CSV from path. Raises FileNotFoundError if not present.\"\"\"\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"{csv_path} not found.\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    return df\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Utility: save DataFrame to SQLite table\n",
        "# ------------------------------------------------------------\n",
        "def save_df_to_db(df, db_path=DEFAULT_DB, table=\"tweets_table\"):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    try:\n",
        "        df.to_sql(table, conn, if_exists=\"replace\", index=False)\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Utility: load table from DB\n",
        "# ------------------------------------------------------------\n",
        "def load_table_from_db(db_path=DEFAULT_DB, table=\"tweets_table\"):\n",
        "    if not os.path.exists(db_path):\n",
        "        raise FileNotFoundError(f\"{db_path} not found.\")\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    try:\n",
        "        df = pd.read_sql_query(f\"SELECT * FROM {table}\", conn)\n",
        "    finally:\n",
        "        conn.close()\n",
        "    return df\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1Ô∏è‚É£ Cleaning function (same logic)\n",
        "# ------------------------------------------------------------\n",
        "def clean_text(text):\n",
        "    \"\"\"Lowercase, remove links, mentions, non-alpha characters.\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2Ô∏è‚É£ Normalization (spaCy lemmatization)\n",
        "# Lazy-load spaCy model on first call to avoid import-time cost.\n",
        "# ------------------------------------------------------------\n",
        "_spacy_nlp = None\n",
        "\n",
        "def _get_spacy():\n",
        "    global _spacy_nlp\n",
        "    if _spacy_nlp is None:\n",
        "        _spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
        "    return _spacy_nlp\n",
        "\n",
        "def token_lemma_nonstop(text):\n",
        "    \"\"\"Tokenize, lemmatize, remove stopwords using spaCy.\"\"\"\n",
        "    nlp = _get_spacy()\n",
        "    doc = nlp(str(text))\n",
        "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3Ô∏è‚É£ Word count with optional multiprocessing\n",
        "# ------------------------------------------------------------\n",
        "def _count_chunk(chunk):\n",
        "    c = Counter()\n",
        "    for t in chunk:\n",
        "        c.update(str(t).split())\n",
        "    return c\n",
        "\n",
        "def count_words_list(texts, num_processes=4):\n",
        "    \"\"\"Return Counter of word frequencies (uses multiprocessing).\"\"\"\n",
        "    if not texts:\n",
        "        return Counter()\n",
        "\n",
        "    n = max(1, num_processes)\n",
        "    chunk_size = max(1, len(texts) // n)\n",
        "    chunks = [texts[i:i+chunk_size] for i in range(0, len(texts), chunk_size)]\n",
        "\n",
        "    if len(chunks) == 1:\n",
        "        results = [_count_chunk(chunks[0])]\n",
        "    else:\n",
        "        with multiprocessing.Pool(processes=n) as pool:\n",
        "            results = pool.map(_count_chunk, chunks)\n",
        "\n",
        "    total = sum(results, Counter())\n",
        "    return total\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4Ô∏è‚É£ VADER Sentiment (parallel-safe for Colab)\n",
        "# ------------------------------------------------------------\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "_sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def _vader_analyze_chunk(chunk):\n",
        "    \"\"\"Helper for multiprocessing ‚Äî process one chunk of text.\"\"\"\n",
        "    out = []\n",
        "    for t in chunk:\n",
        "        score = _sia.polarity_scores(str(t))[\"compound\"]\n",
        "        if score > 0.05:\n",
        "            label = \"positive\"\n",
        "        elif score < -0.05:\n",
        "            label = \"negative\"\n",
        "        else:\n",
        "            label = \"neutral\"\n",
        "        out.append((score, label))\n",
        "    return out\n",
        "\n",
        "def vader_sentiment_texts(texts, num_processes=4):\n",
        "    \"\"\"Given list of texts, return list of (score, label) tuples in same order.\"\"\"\n",
        "    if not texts:\n",
        "        return []\n",
        "\n",
        "    n = max(1, num_processes)\n",
        "    chunk_size = max(1, len(texts) // n)\n",
        "    chunks = [texts[i:i + chunk_size] for i in range(0, len(texts), chunk_size)]\n",
        "\n",
        "    if len(chunks) == 1:\n",
        "        results = [_vader_analyze_chunk(chunks[0])]\n",
        "    else:\n",
        "        with multiprocessing.Pool(processes=n) as pool:\n",
        "            results = pool.map(_vader_analyze_chunk, chunks)\n",
        "\n",
        "    # flatten\n",
        "    flat = [item for sub in results for item in sub]\n",
        "    return flat\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5Ô∏è‚É£ Save sentiment results to DB\n",
        "# ------------------------------------------------------------\n",
        "def save_sentiment_results(df, db_path=DEFAULT_DB, table=\"tweets_table_sentiment\"):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    try:\n",
        "        df.to_sql(table, conn, if_exists=\"replace\", index=False)\n",
        "    finally:\n",
        "        conn.close()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6Ô∏è‚É£ Plot helpers: bar and pie (return fig object)\n",
        "# ------------------------------------------------------------\n",
        "def plot_top_words(counter, top_n=10):\n",
        "    top = counter.most_common(top_n)\n",
        "    if not top:\n",
        "        fig = plt.figure()\n",
        "        return fig\n",
        "    words, counts = zip(*top)\n",
        "    fig, ax = plt.subplots(figsize=(10,5))\n",
        "    ax.bar(words, counts, edgecolor=\"black\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.xlabel(\"Words\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(f\"Top {top_n} Most Frequent Words\")\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_sentiment_pie(labels_series, save_path=SENT_PIE_PATH):\n",
        "    counts = labels_series.value_counts()\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    ax.pie(counts, labels=counts.index, autopct=\"%1.1f%%\", startangle=140,\n",
        "           colors=[\"lightgreen\",\"salmon\",\"lightblue\"])\n",
        "    ax.set_title(\"Sentiment Distribution (VADER)\")\n",
        "    fig.savefig(save_path, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "    return save_path\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7Ô∏è‚É£ Email helpers (image or csv)\n",
        "# ------------------------------------------------------------\n",
        "def send_email_alert(to_email, subject, message, image_path=None,\n",
        "                     smtp_server=\"smtp.gmail.com\", smtp_port=587,\n",
        "                     sender_email=None, sender_password=None):\n",
        "    \"\"\"\n",
        "    Send an email with optional image attachment.\n",
        "    Note: supply sender_email and sender_password (app password) when calling.\n",
        "    \"\"\"\n",
        "    if sender_email is None or sender_password is None:\n",
        "        raise ValueError(\"sender_email and sender_password must be provided\")\n",
        "\n",
        "    msg = MIMEMultipart()\n",
        "    msg[\"From\"] = sender_email\n",
        "    msg[\"To\"] = to_email\n",
        "    msg[\"Subject\"] = subject\n",
        "    msg.attach(MIMEText(message, \"plain\"))\n",
        "\n",
        "    if image_path and os.path.exists(image_path):\n",
        "        with open(image_path, \"rb\") as f:\n",
        "            img = MIMEImage(f.read())\n",
        "            img.add_header('Content-Disposition', 'attachment', filename=os.path.basename(image_path))\n",
        "            msg.attach(img)\n",
        "\n",
        "    server = smtplib.SMTP(smtp_server, smtp_port)\n",
        "    try:\n",
        "        server.starttls()\n",
        "        server.login(sender_email, sender_password)\n",
        "        server.sendmail(sender_email, to_email, msg.as_string())\n",
        "    finally:\n",
        "        server.quit()\n",
        "\n",
        "def send_email_with_csv(to_email, subject, message, file_path=None,\n",
        "                        smtp_server=\"smtp.gmail.com\", smtp_port=587,\n",
        "                        sender_email=None, sender_password=None):\n",
        "    \"\"\"\n",
        "    Send email with CSV attachment.\n",
        "    \"\"\"\n",
        "    if sender_email is None or sender_password is None:\n",
        "        raise ValueError(\"sender_email and sender_password must be provided\")\n",
        "\n",
        "    msg = MIMEMultipart()\n",
        "    msg[\"From\"] = sender_email\n",
        "    msg[\"To\"] = to_email\n",
        "    msg[\"Subject\"] = subject\n",
        "    msg.attach(MIMEText(message, \"plain\"))\n",
        "\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            part = MIMEBase(\"application\", \"octet-stream\")\n",
        "            part.set_payload(f.read())\n",
        "        encoders.encode_base64(part)\n",
        "        part.add_header('Content-Disposition', f'attachment; filename=\"{os.path.basename(file_path)}\"')\n",
        "        msg.attach(part)\n",
        "\n",
        "    server = smtplib.SMTP(smtp_server, smtp_port)\n",
        "    try:\n",
        "        server.starttls()\n",
        "        server.login(sender_email, sender_password)\n",
        "        server.sendmail(sender_email, to_email, msg.as_string())\n",
        "    finally:\n",
        "        server.quit()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8Ô∏è‚É£ Hugging Face sentiment (modern)\n",
        "# ------------------------------------------------------------\n",
        "# ------------------------------------------------------------\n",
        "# 8Ô∏è‚É£ Hugging Face sentiment (optimized for Colab / Streamlit)\n",
        "# ------------------------------------------------------------\n",
        "from transformers import pipeline\n",
        "\n",
        "def huggingface_sentiment(texts, model_name=\"cardiffnlp/twitter-roberta-base-sentiment\", device=None, batch_size=32):\n",
        "    \"\"\"\n",
        "    Run Hugging Face sentiment analysis efficiently (no multiprocessing).\n",
        "    Handles batching internally and avoids heavy parallel model loads.\n",
        "    Returns list of dicts like {'label':..., 'score':...}.\n",
        "    \"\"\"\n",
        "    if not texts:\n",
        "        return []\n",
        "\n",
        "    if device is None:\n",
        "        device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "    # load pipeline once\n",
        "    pipe = pipeline(\"sentiment-analysis\", model=model_name, device=device)\n",
        "\n",
        "    results = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        res = pipe(batch, truncation=True)\n",
        "        label_map = {\"LABEL_0\": \"Negative\", \"LABEL_1\": \"Neutral\", \"LABEL_2\": \"Positive\"}\n",
        "        mapped = [{\"label\": label_map.get(r[\"label\"], r[\"label\"]), \"score\": r[\"score\"]} for r in res]\n",
        "        results.extend(mapped)\n",
        "\n",
        "    return results\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 9Ô∏è‚É£ Agreement check & confusion table\n",
        "# ------------------------------------------------------------\n",
        "def agreement_table(df, col_a=\"sentiment_label\", col_b=\"modern_label\"):\n",
        "    if col_a not in df.columns or col_b not in df.columns:\n",
        "        raise KeyError(f\"Columns {col_a} and/or {col_b} not in DataFrame\")\n",
        "    agreement = (df[col_a] == df[col_b]).mean()\n",
        "    table = pd.crosstab(df[col_a], df[col_b])\n",
        "    return agreement, table\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 10Ô∏è‚É£ Convenience: full pipeline runner (only when executed directly)\n",
        "# ------------------------------------------------------------\n",
        "def run_full_pipeline(csv_path=DEFAULT_CSV, db_path=DEFAULT_DB, sender_email=None, sender_password=None):\n",
        "    \"\"\"\n",
        "    Runs the full pipeline similar to your original script.\n",
        "    Saves intermediate artifacts and returns final dataframe.\n",
        "    \"\"\"\n",
        "    # 1. load\n",
        "    df = load_csv(csv_path)\n",
        "\n",
        "    # 2. save to db\n",
        "    save_df_to_db(df, db_path=db_path, table=\"tweets_table\")\n",
        "\n",
        "    # 3. cleaning\n",
        "    df[\"cleaned\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "    # 4. normalization\n",
        "    df[\"normalized\"] = df[\"cleaned\"].apply(token_lemma_nonstop)\n",
        "\n",
        "    # 5. word counts\n",
        "    texts_norm = df[\"normalized\"].astype(str).tolist()\n",
        "    total_counts = count_words_list(texts_norm, num_processes=4)\n",
        "\n",
        "    # 6. vader sentiment\n",
        "    vader_results = vader_sentiment_texts(df[\"text\"].astype(str).tolist(), num_processes=4)\n",
        "    if vader_results:\n",
        "        df[\"sentiment_score\"], df[\"sentiment_label\"] = zip(*vader_results)\n",
        "    else:\n",
        "        df[\"sentiment_score\"], df[\"sentiment_label\"] = [], []\n",
        "\n",
        "    # 7. save sentiment db and pie\n",
        "    save_sentiment_results(df, db_path=db_path, table=\"tweets_table_sentiment\")\n",
        "    plot_sentiment_pie(df[\"sentiment_label\"], save_path=SENT_PIE_PATH)\n",
        "\n",
        "    # 8. hugging face sentiment\n",
        "    hf_results = huggingface_sentiment(df[\"text\"].astype(str).tolist())\n",
        "    df[\"modern_label\"] = [r[\"label\"] for r in hf_results]\n",
        "    df[\"modern_score\"] = [r[\"score\"] for r in hf_results]\n",
        "\n",
        "    # ‚úÖ Rename columns for consistency with UI\n",
        "    df.rename(columns={\n",
        "        \"sentiment_label\": \"vader_label\",\n",
        "        \"modern_label\": \"hf_label\"\n",
        "    }, inplace=True)\n",
        "\n",
        "    # 9. save CSV output\n",
        "    df.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "    # 10. optionally email results (if credentials provided)\n",
        "    summary_lines = [\n",
        "        \"üìä Twitter Sentiment Report\\n\\n\",\n",
        "        f\"Total tweets analyzed: {len(df)}\\n\",\n",
        "        f\"Sentiment counts: {df['sentiment_label'].value_counts().to_dict()}\\n\\n\",\n",
        "        f\"Top words (normalized): {total_counts.most_common(10)}\\n\\n\"\n",
        "    ]\n",
        "    summary_text = \"\".join(summary_lines)\n",
        "\n",
        "    if sender_email and sender_password:\n",
        "        try:\n",
        "            send_email_alert(sender_email, \"Twitter Sentiment Report\", summary_text, image_path=SENT_PIE_PATH,\n",
        "                             sender_email=sender_email, sender_password=sender_password)\n",
        "        except Exception as e:\n",
        "            print(\"Email send failed:\", e)\n",
        "\n",
        "    return df\n",
        "\n",
        "def build_final_results(db_path=DEFAULT_DB, output_csv=OUTPUT_CSV):\n",
        "    \"\"\"\n",
        "    Merge VADER and Hugging Face results (if available) into final CSV.\n",
        "    Returns DataFrame of merged results.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(db_path):\n",
        "        raise FileNotFoundError(\"Database not found. Please run analysis first.\")\n",
        "\n",
        "    try:\n",
        "        df_vader = load_table_from_db(db_path, table=\"tweets_vader\")\n",
        "        df_hf = load_table_from_db(db_path, table=\"tweets_hf\")\n",
        "        df_final = pd.merge(df_vader, df_hf, on=\"text\", suffixes=(\"_vader\", \"_hf\"))\n",
        "        df_final.to_csv(output_csv, index=False)\n",
        "        return df_final\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to build final results: {e}\")\n",
        "# ------------------------------------------------------------\n",
        "# Run full pipeline only when file executed directly\n",
        "# ------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # If user runs `python backend.py`, attempt to run full pipeline with defaults.\n",
        "    try:\n",
        "        print(\"Running full backend pipeline (from backend.py)...\")\n",
        "        run_full_pipeline()\n",
        "        print(\"Done.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error running full pipeline:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIh-QN8gWV-n",
        "outputId": "89fa2a50-1ea7-4226-a3e9-16355b3f12f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "# ============================================================\n",
        "# üåü Twitter Sentiment Analyzer ‚Äî Modern Streamlit UI\n",
        "# ============================================================\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sqlite3\n",
        "import os   # ‚úÖ Added for Colab secret access\n",
        "from io import BytesIO\n",
        "from backend import (\n",
        "    clean_text,\n",
        "    token_lemma_nonstop,\n",
        "    count_words_list,\n",
        "    vader_sentiment_texts,\n",
        "    huggingface_sentiment,\n",
        "    plot_top_words,\n",
        "    plot_sentiment_pie,\n",
        "    send_email_with_csv,\n",
        "    save_df_to_db,\n",
        "    load_table_from_db,\n",
        "    save_sentiment_results,\n",
        "    agreement_table,\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# PAGE CONFIG\n",
        "# ============================================================\n",
        "st.set_page_config(page_title=\"Twitter Sentiment Analyzer\", layout=\"wide\")\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    .main-title {\n",
        "        font-size: 2.2rem;\n",
        "        color: #2E8B57;\n",
        "        text-align: center;\n",
        "        font-weight: 700;\n",
        "        margin-bottom: 0.3rem;\n",
        "    }\n",
        "    .sub-title {\n",
        "        text-align: center;\n",
        "        color: #444;\n",
        "        margin-bottom: 1.5rem;\n",
        "    }\n",
        "    .footer {\n",
        "        text-align: center;\n",
        "        margin-top: 2rem;\n",
        "        color: gray;\n",
        "        font-size: 0.9rem;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "\n",
        "st.markdown('<div class=\"main-title\">üß† Twitter Sentiment Analyzer</div>', unsafe_allow_html=True)\n",
        "st.markdown('<div class=\"sub-title\">Upload ‚Üí Clean ‚Üí Analyze ‚Üí Compare ‚Üí Download or Email Results</div>', unsafe_allow_html=True)\n",
        "\n",
        "DB_PATH = \"tweets.db\"\n",
        "RESULT_CSV = \"Milestone3_Final_Results.csv\"\n",
        "\n",
        "# ============================================================\n",
        "# MAIN APP\n",
        "# ============================================================\n",
        "tabs = st.tabs([\"üìÅ Upload Data\", \"üßπ Clean & Normalize\", \"üìä Top Words\", \"ü§ñ Sentiment Analysis\", \"‚öñÔ∏è Comparison\", \"üìß Report\"])\n",
        "\n",
        "# ============================================================\n",
        "# TAB 1: UPLOAD\n",
        "# ============================================================\n",
        "with tabs[0]:\n",
        "    st.header(\"üìÅ Upload Your CSV File\")\n",
        "    uploaded_file = st.file_uploader(\"Upload tweetsData.csv\", type=[\"csv\"])\n",
        "\n",
        "    if uploaded_file:\n",
        "        df = pd.read_csv(uploaded_file)\n",
        "        st.success(f\"‚úÖ File uploaded successfully ‚Äî {len(df)} rows, {len(df.columns)} columns\")\n",
        "\n",
        "        # Save to DB\n",
        "        save_df_to_db(df, DB_PATH, table=\"tweets_table\")\n",
        "\n",
        "        # Preview section\n",
        "        st.subheader(\"üëÄ Data Preview\")\n",
        "        num_rows = st.slider(\"Select number of rows to display\", 5, 50, 10)\n",
        "        st.dataframe(df.head(num_rows), use_container_width=True)\n",
        "\n",
        "        st.info(\"Next: Go to the 'Clean & Normalize' tab to preprocess text.\")\n",
        "    else:\n",
        "        st.warning(\"Please upload a CSV file to continue.\")\n",
        "\n",
        "# ============================================================\n",
        "# TAB 2: CLEAN & NORMALIZE (Fixed ‚Äì cleaned preview persists)\n",
        "# ============================================================\n",
        "with tabs[1]:\n",
        "    st.header(\"üßπ Clean & Normalize Text \")\n",
        "    if os.path.exists(DB_PATH):\n",
        "        df = load_table_from_db(DB_PATH)\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        # Initialize session states if missing\n",
        "        if \"cleaned_df\" not in st.session_state:\n",
        "            st.session_state.cleaned_df = None\n",
        "        if \"normalized_df\" not in st.session_state:\n",
        "            st.session_state.normalized_df = None\n",
        "\n",
        "        with col1:\n",
        "            if st.button(\"üßº Clean Text\"):\n",
        "                with st.spinner(\"Cleaning tweets...\"):\n",
        "                    df[\"cleaned\"] = df[\"text\"].apply(clean_text)\n",
        "                    save_df_to_db(df, DB_PATH, table=\"tweets_table_cleaned\")\n",
        "                    st.session_state.cleaned_df = df[[\"text\", \"cleaned\"]]\n",
        "                st.success(\"‚úÖ Text cleaned successfully!\")\n",
        "\n",
        "        with col2:\n",
        "            if st.button(\"Normalize Text\"):\n",
        "                with st.spinner(\"Normalizing text...\"):\n",
        "                    # Make sure cleaned data exists\n",
        "                    if \"cleaned\" not in df.columns:\n",
        "                        if st.session_state.cleaned_df is not None:\n",
        "                            df = st.session_state.cleaned_df.copy()\n",
        "                        else:\n",
        "                            st.warning(\"Cleaned text not found ‚Äî please clean first.\")\n",
        "                            st.stop()\n",
        "\n",
        "                    df[\"normalized\"] = df[\"cleaned\"].apply(token_lemma_nonstop)\n",
        "                    save_df_to_db(df, DB_PATH, table=\"tweets_table_cleaned\")\n",
        "                    st.session_state.normalized_df = df[[\"text\", \"cleaned\", \"normalized\"]]\n",
        "                st.success(\"‚úÖ Text normalized successfully!\")\n",
        "\n",
        "        # Always show previews if available\n",
        "        if st.session_state.cleaned_df is not None:\n",
        "            st.subheader(\"üßº Cleaned Text Preview\")\n",
        "            st.dataframe(st.session_state.cleaned_df.head(10), width=\"stretch\")\n",
        "\n",
        "        if st.session_state.normalized_df is not None:\n",
        "            st.subheader(\"üî§ Normalized Text Preview\")\n",
        "            st.dataframe(st.session_state.normalized_df.head(10), width=\"stretch\")\n",
        "\n",
        "    else:\n",
        "        st.warning(\"‚ö†Ô∏è Please upload data first in the 'Upload Data' tab.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TAB 3: TOP WORDS (FULLY FIXED)\n",
        "# ============================================================\n",
        "with tabs[2]:\n",
        "    st.header(\"üìä View Top Frequent Words\")\n",
        "    if os.path.exists(DB_PATH):\n",
        "        # Load cleaned/normalized table explicitly\n",
        "        df = load_table_from_db(DB_PATH, table=\"tweets_table_cleaned\")\n",
        "\n",
        "        if \"normalized\" in df.columns and not df[\"normalized\"].isna().all():\n",
        "            st.info(\"‚úÖ Using normalized text for word frequency analysis.\")\n",
        "            top_n = st.number_input(\"Number of top words to display\", min_value=5, max_value=50, value=10)\n",
        "            if st.button(\"Show Top Words\"):\n",
        "                with st.spinner(\"Counting top words...\"):\n",
        "                    texts_norm = df[\"normalized\"].astype(str).tolist()\n",
        "                    total_counts = count_words_list(texts_norm)\n",
        "                    fig = plot_top_words(total_counts, top_n=top_n)\n",
        "                    st.pyplot(fig)\n",
        "        else:\n",
        "            st.warning(\"‚ö†Ô∏è Please normalize the text first in the previous tab.\")\n",
        "    else:\n",
        "        st.warning(\"‚ö†Ô∏è Please upload and clean your data first.\")\n",
        "\n",
        "# ============================================================\n",
        "# TAB 4: SENTIMENT ANALYSIS\n",
        "# ============================================================\n",
        "with tabs[3]:\n",
        "    st.header(\"ü§ñ Sentiment Analysis Options\")\n",
        "\n",
        "    if os.path.exists(DB_PATH):\n",
        "        df = load_table_from_db(DB_PATH)\n",
        "        sub_tabs = st.tabs([\"‚öôÔ∏è VADER Analysis\", \"ü§ñ Hugging Face Analysis\"])\n",
        "\n",
        "        # ---------- VADER ----------\n",
        "        with sub_tabs[0]:\n",
        "            if st.button(\"Run VADER Sentiment Analysis\"):\n",
        "                with st.spinner(\"Running VADER sentiment...\"):\n",
        "                    results = vader_sentiment_texts(df[\"text\"].astype(str).tolist(), num_processes=4)\n",
        "                    if results:\n",
        "                        df[\"vader_score\"], df[\"vader_label\"] = zip(*results)\n",
        "                        save_sentiment_results(df, DB_PATH, table=\"tweets_vader\")\n",
        "                        pie_path = plot_sentiment_pie(df[\"vader_label\"])\n",
        "                        st.image(pie_path, caption=\"VADER Sentiment Distribution\")\n",
        "                        st.dataframe(df[[\"text\", \"vader_label\", \"vader_score\"]].head(10))\n",
        "                        st.success(\"‚úÖ VADER Sentiment Analysis Completed!\")\n",
        "                        df.to_csv(\"vader_results.csv\", index=False)\n",
        "                        st.download_button(\"üì• Download VADER Results\", data=open(\"vader_results.csv\", \"rb\"), file_name=\"vader_results.csv\")\n",
        "                    else:\n",
        "                        st.error(\"No sentiment results found.\")\n",
        "\n",
        "        # ---------- HUGGING FACE ----------\n",
        "        with sub_tabs[1]:\n",
        "            if st.button(\"Run Hugging Face Sentiment Analysis\"):\n",
        "                with st.spinner(\"Running Hugging Face model... this may take a minute ‚è≥\"):\n",
        "                    results = huggingface_sentiment(df[\"text\"].astype(str).tolist())\n",
        "                    df[\"hf_label\"] = [r[\"label\"] for r in results]\n",
        "                    df[\"hf_score\"] = [r[\"score\"] for r in results]\n",
        "                    save_sentiment_results(df, DB_PATH, table=\"tweets_hf\")\n",
        "                    pie_path = plot_sentiment_pie(df[\"hf_label\"])\n",
        "                    st.image(pie_path, caption=\"Hugging Face Sentiment Distribution\")\n",
        "                    st.dataframe(df[[\"text\", \"hf_label\", \"hf_score\"]].head(10))\n",
        "                    st.success(\"‚úÖ Hugging Face Sentiment Completed!\")\n",
        "                    df.to_csv(\"hf_results.csv\", index=False)\n",
        "                    st.download_button(\"üì• Download HF Results\", data=open(\"hf_results.csv\", \"rb\"), file_name=\"hf_results.csv\")\n",
        "    else:\n",
        "        st.warning(\"Please upload and clean your data first.\")\n",
        "\n",
        "# ============================================================\n",
        "# TAB 5: COMPARISON (FIXED)\n",
        "# ============================================================\n",
        "with tabs[4]:\n",
        "    st.header(\"‚öñÔ∏è Compare VADER vs Hugging Face\")\n",
        "\n",
        "    if os.path.exists(DB_PATH):\n",
        "        try:\n",
        "            # Load both sentiment tables\n",
        "            df_vader = load_table_from_db(DB_PATH, table=\"tweets_vader\")\n",
        "            df_hf = load_table_from_db(DB_PATH, table=\"tweets_hf\")\n",
        "\n",
        "            # Merge on 'text' column (assuming tweets are same)\n",
        "            df_merged = pd.merge(df_vader, df_hf, on=\"text\", suffixes=(\"_vader\", \"_hf\"))\n",
        "\n",
        "            if \"vader_label\" in df_merged.columns and \"hf_label\" in df_merged.columns:\n",
        "                agreement, table = agreement_table(df_merged, col_a=\"vader_label\", col_b=\"hf_label\")\n",
        "                st.metric(label=\"Model Agreement (%)\", value=f\"{agreement*100:.2f}\")\n",
        "\n",
        "                fig, ax = plt.subplots(figsize=(6, 4))\n",
        "                sns.heatmap(table, annot=True, fmt=\"d\", cmap=\"Greens\", ax=ax)\n",
        "                plt.title(\"VADER vs HF Sentiment Comparison\")\n",
        "                st.pyplot(fig)\n",
        "\n",
        "                st.download_button(\n",
        "                    \"üì• Download Comparison Table\",\n",
        "                    data=table.to_csv().encode('utf-8'),\n",
        "                    file_name=\"comparison_table.csv\"\n",
        "                )\n",
        "            else:\n",
        "                st.warning(\"Please run both sentiment analyses first.\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error loading comparison: {e}\")\n",
        "    else:\n",
        "        st.warning(\"Please complete previous steps first.\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TAB 6: REPORT / EMAIL (Updated Secure Email)\n",
        "# ============================================================\n",
        "with tabs[5]:\n",
        "    st.header(\"üìß Download or Email Your Results\")\n",
        "\n",
        "    if os.path.exists(DB_PATH):\n",
        "        try:\n",
        "            from backend import build_final_results\n",
        "\n",
        "            # Use backend function to generate final CSV\n",
        "            df_final = build_final_results(DB_PATH, RESULT_CSV)\n",
        "            st.success(f\"‚úÖ Final report ready with {len(df_final)} tweets!\")\n",
        "\n",
        "            # -------------------------------\n",
        "            # Download Section\n",
        "            # -------------------------------\n",
        "            st.markdown(\"### üì• Download Final CSV\")\n",
        "            with open(RESULT_CSV, \"rb\") as f:\n",
        "                st.download_button(\"Download File\", f, file_name=RESULT_CSV)\n",
        "\n",
        "            # -------------------------------\n",
        "            # Email Section (Secure)\n",
        "            # -------------------------------\n",
        "            st.markdown(\"### üì© Email Report\")\n",
        "            email = st.text_input(\"Enter your email address:\")\n",
        "            attach_chart = st.checkbox(\"Attach Sentiment Pie Chart\")\n",
        "\n",
        "            if st.button(\"Send Email Report\"):\n",
        "                if not email:\n",
        "                    st.error(\"Please enter a valid email address.\")\n",
        "                else:\n",
        "                    msg = f\"Twitter Sentiment Report ‚Äî {len(df_final)} Tweets analyzed successfully!\"\n",
        "                    image_path = \"sentiment_pie.png\" if attach_chart and os.path.exists(\"sentiment_pie.png\") else None\n",
        "\n",
        "                    # ‚úÖ Secure method: Load Gmail password from Colab Secret\n",
        "                    SENDER_EMAIL = \"muskansoni0524@gmail.com\"\n",
        "                    SENDER_PASSWORD = os.environ.get(\"gmail_password\")\n",
        "\n",
        "                    if not SENDER_PASSWORD:\n",
        "                        st.error(\"‚ö†Ô∏è Gmail password secret not found. Please add it as a Colab secret named 'gmail_password'.\")\n",
        "                    else:\n",
        "                        try:\n",
        "                            send_email_with_csv(\n",
        "                                to_email=email,\n",
        "                                subject=\"Twitter Sentiment Report\",\n",
        "                                message=msg,\n",
        "                                file_path=RESULT_CSV,\n",
        "                                sender_email=SENDER_EMAIL,\n",
        "                                sender_password=SENDER_PASSWORD\n",
        "                            )\n",
        "                            st.success(f\"‚úÖ Report emailed successfully to {email}\")\n",
        "                        except Exception as e:\n",
        "                            st.error(f\"‚ùå Failed to send email: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.warning(f\"‚ö†Ô∏è Could not prepare final report: {e}\")\n",
        "            st.info(\"Please ensure both VADER and Hugging Face analyses were run.\")\n",
        "    else:\n",
        "        st.warning(\"‚ö†Ô∏è Please upload and analyze your data before generating a report.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "svMfQyH9XS_i"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit pyngrok --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAPSEEYuXkBA",
        "outputId": "4f3c0478-f320-43bb-f949-26a2c95b7845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåç Your app is live at: NgrokTunnel: \"https://unstated-rudolf-unbrazenly.ngrok-free.dev\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.11.229.54:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-11-05 15:19:08.945822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1762355948.967281   26428 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1762355948.973761   26428 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1762355948.991099   26428 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762355948.991125   26428 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762355948.991129   26428 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1762355948.991132   26428 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-05 15:19:08.996166: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-05 15:19:35.472 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-05 15:19:41.973 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-05 15:19:45.912 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-05 15:23:14.128 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-05 15:23:21.513 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "2025-11-05 15:23:34.992 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
            "Device set to use cuda:0\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "2025-11-05 15:28:32.413 Please replace `use_container_width` with `width`.\n",
            "\n",
            "`use_container_width` will be removed after 2025-12-31.\n",
            "\n",
            "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# üîë STEP 1 ‚Äî Set your ngrok auth token (only once per runtime)\n",
        "ngrok.set_auth_token(\"34T9eLS4rcH76eKVZwXEKFv4uLa_456R8DWTQcoGd5govmc7o\")  # üëà paste your token here\n",
        "\n",
        "# üõë STEP 2 ‚Äî Kill any existing tunnels (safety)\n",
        "ngrok.kill()\n",
        "\n",
        "# üåç STEP 3 ‚Äî Create a new tunnel to Streamlit's default port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"üåç Your app is live at: {public_url}\")\n",
        "\n",
        "# üöÄ STEP 4 ‚Äî Run your Streamlit app\n",
        "!streamlit run app.py --server.port 8501\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}